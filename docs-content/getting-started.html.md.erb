---
title: Getting Started with Spring Cloud&reg; Data Flow for PCF
owner: Spring Cloud Services
---

<strong><%= modified_date %></strong>

See below for steps to get started using Spring Cloud Data Flow for PCF. The examples below use Spring Cloud Data Flow for PCF to quickly create a data pipeline.

<p class='note'><strong>Note</strong>: In order to have read and write access to a Spring Cloud Data Flow for PCF service instance, you must have the <code>SpaceDeveloper</code> role in the space where the service instance was created. If you have only the <code>SpaceAuditor</code> role in the space where the service instance was created, you have only read (not write) access to the service instance.</p>

<p class='tip'>Consider installing the Spring Cloud Data Flow for PCF and Service Instance Logs cf CLI plugins. See the <a href="using-the-shell.html">Using the Shell</a> and <a href="viewing-service-instance-logs.html">Viewing Service Instance Logs</a> topics. <br/> <br/> The examples in this topic use the Spring Cloud Data Flow for PCF cf CLI plugin.</p>

## <a id="creating-a-data-pipeline-using-the-shell"></a>Creating a Data Pipeline Using the Shell

Create a Spring Cloud Data Flow service instance (see the [Creating an Instance](managing-service-instances.html#creating) section of the [Managing Service Instances](managing-service-instances.html) topic). If you use the default backing data services of MySQL for PCF, RabbitMQ for PCF, and Redis for PCF, you can then import the Spring Cloud Data Flow OSS "RabbitMQ + Maven" [stream app starters](https://cloud.spring.io/spring-cloud-stream-app-starters/#stream-app-starters-and-spring-cloud-data-flow-).

Start the Data Flow shell using the `cf dataflow-shell` command added by the Spring Cloud Data Flow for PCF cf CLI plugin:

<pre class="terminal">
$ cf dataflow-shell data-flow
Attaching shell to dataflow service data-flow in org myorg / space development as user...
...
Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:>
</pre>

Import the stream app starters using the Data Flow shell's `app import` command:

<pre class="terminal">
dataflow:>app import http://bit.ly/Darwin-SR3-stream-applications-rabbit-maven
Successfully registered 64 applications from [source.sftp, source.mqtt.metadata,...
processor.scriptable-transform.metadata, processor.object-detection]
</pre>

With the app starters imported, you can use three--the `http` source, the `split` processor, and the `log` sink--to create a stream that consumes data via an HTTP POST request, processes it by splitting it into words, and outputs the results in logs.

Create the stream using the Data Flow shell's `stream create` command:

<pre class="terminal">
dataflow:>stream create --name words --definition "http | splitter --expression=payload.split(' ') | log"
Created new stream 'words'
</pre>

Next, deploy the stream, using the `stream deploy` command:

<pre class="terminal">
dataflow:>stream deploy words
Deployment request has been sent for stream 'words'
</pre>

## <a id="creating-a-data-pipeline-using-the-dashboard"></a>Creating a Data Pipeline Using the Dashboard

Create a Spring Cloud Data Flow service instance (see the [Creating an Instance](managing-service-instances.html#creating) section of the [Managing Service Instances](managing-service-instances.html) topic). If you use the default backing data services of MySQL for PCF, RabbitMQ for PCF, and Redis for PCF, you can then import the Spring Cloud Data Flow OSS "RabbitMQ + Maven" [stream app starters](https://cloud.spring.io/spring-cloud-stream-app-starters/#stream-app-starters-and-spring-cloud-data-flow-).

In Apps Manager, visit the Spring Cloud Data Flow service instance's page and click **Manage** to access its dashboard.

![Service Instance Page](df-manage-link.png)

This will take you to the dashboard's **Apps** tab, where you can import applications.

![Dashboard Apps Tab, With No Apps Imported](dashboard-apps-tab-no-apps.png)

Click **Add Application(s)**.

![Select Import Option](select-import-option.png)

To import the "RabbitMQ + Maven" stream app starters, select the **Bulk import application coordinates from an HTTP URI location** option.

![Bulk Importing Applications](bulk-importing-applications.png)

In the **URI** field, enter `http://bit.ly/Darwin-SR3-stream-applications-rabbit-maven`. Then click **Import the application(s)**.

With the app starters imported, visit the **Streams** tab. You can use three of the imported starter applications---the `http` source, the `split` processor, and the `log` sink---to create a stream that consumes data via an HTTP POST request, processes it by splitting it into words, and outputs the results in logs.

![Dashboard Streams Tab, With No Streams Created](dashboard-streams-tab-no-streams.png)

Click **Create stream(s)** to enter the stream creation view. In the view's left sidebar, search for the `http` source application. Click it and drag it onto the canvas to begin defining a stream.

![Dashboard Streams Tab: Creating a Stream](dashboard-creating-stream-1.png)

Search for and add the `splitter` processor application and `log` sink application.

![Dashboard Streams Tab: Adding Applications to a Stream](dashboard-creating-stream-2.png)

Click the `splitter` application, then click the gear icon beside it to edit its properties. In the **expression** field, enter `payload.split(' ')`. Click **OK**.

![Dashboard Streams Tab: Editing an Application's Properties](dashboard-edit-application-properties.png)

Click and drag between the output and input ports on the applications to connect them and complete the stream.

![Dashboard Streams Tab: A Completed Stream](dashboard-completed-stream.png)

Click the **Create Stream** button. Type the name "words", then click **Create the stream**.

![Dashboard Streams Tab: Naming a Stream](dashboard-naming-stream.png)

The **Streams** tab now displays the new stream. Click the **&#x25BA;** button to deploy the stream.

![Dashboard Streams Tab, With A Stream](dashboard-after-stream-created.png)

Click **Deploy the stream**.

![Dashboard Streams Tab: Deploying a Stream](dashboard-deploying-stream.png)

## <a id="using-the-deployed-data-pipeline"></a>Using the Deployed Data Pipeline

You can run the `cf apps` command to see the applications deployed as part of the stream:

<pre class="terminal">
$ cf apps
Getting apps in org myorg / space development as user...
OK

name                                   requested state   instances   memory   disk   urls
QiK4v3p-words-log-v1                   started           1/1         1G       1G     QiK4v3p-words-log-v1.apps.example.com
QiK4v3p-words-splitter-v1              started           1/1         1G       1G     QiK4v3p-words-splitter-v1.apps.example.com
QiK4v3p-words-http-v1                  started           0/1         1G       1G     QiK4v3p-words-http-v1.apps.example.com
</pre>

Run the `cf logs` command on the `QiK4v3p-words-log-v1` application:

<pre class="terminal">
$ cf logs QiK4v3p-words-log-v1
Retrieving logs for app QiK4v3p-words-log-v1 in org myorg / space development as user...
</pre>

Then, in a separate command line, use the Data Flow shell (started using the `cf dataflow-shell` command) to send a POST request to the `QiK4v3p-words-http-v1` application:

<pre class="terminal">
dataflow:>http post --target http://QiK4v3p-words-http-v1.apps.example.com --data "This is a test"
> POST (text/plain) http://QiK4v3p-words-http-v1.apps.example.com This is a test
> 202 ACCEPTED
</pre>

Watch for the processed data in the `QiK4v3p-words-log-v1` application's logs:

<pre class="terminal">
2019-03-22T10:53:58.60-0500 [APP/PROC/WEB/0] OUT 2019-03-22 15:53:58.608  INFO 19 --- [plitter.words-1] QiK4v3p-words-log-v1 : This
2019-03-22T10:53:58.65-0500 [APP/PROC/WEB/0] OUT 2019-03-22 15:53:58.650  INFO 19 --- [plitter.words-1] QiK4v3p-words-log-v1 : is
2019-03-22T10:53:58.65-0500 [APP/PROC/WEB/0] OUT 2019-03-22 15:53:58.655  INFO 19 --- [plitter.words-1] QiK4v3p-words-log-v1 : a
2019-03-22T10:53:58.66-0500 [APP/PROC/WEB/0] OUT 2019-03-22 15:53:58.660  INFO 19 --- [plitter.words-1] QiK4v3p-words-log-v1 : test
</pre>
